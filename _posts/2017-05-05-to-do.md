---
layout:     post
title:      "No Title"
date:       2017-05-05
summary:    Playing around with books from project Gutenberg.
categories: nlp ml
custom_js:
    - katex
---

### Prologue: In Which the Author Explains Why He's Doing What He's Doing

Books are fun. <sup>[[citation needed](https://xkcd.com/285)]</sup> 

What's even more fun are vector space models, clustering algorithms, and dimensionality reduction techniques. In this blog post, we're going to combine it all by playing around with a small set of texts from project Gutenberg. With a bit of luck, Python, and lots of trial and error, we might just learn something interesting.

### Chapter One: In Which Books are Fetched and Puns are Made
We should start by fetching some books. There are many ways to do it, but for starters let's just use what NLTK has to offer: 

```python
from nltk.corpus import gutenberg
titles = gutenberg.fileids()
print(titles)
```
```
['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt',
 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt',
 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt',
 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt',
 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt',
 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']
```

This rather eclectic collection will serve as our dataset. We can weed out some boring books (your definition of boring may vary) and fetch the full text for the rest. Let's also be pedantic and use some regexp magic to format the titles:

```python
boring = {'bible-kjv.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt'} 
titles = [t for t in titles if t not in boring] 
texts = [gutenberg.raw(t) for t in titles] 
titles = [re.search(r'-(.*?)\.', title).group(1) for title in titles] 
print(titles)
```
```
['emma', 'persuasion', 'sense', 'poems', 'stories', 'busterbrown', 'alice', 'ball', 'brown', 'thursday', 'paradise', 'caesar', 'hamlet', 'macbeth', 'leaves']
```

Conveniently, and completely coincidentally, the remaining titles fall into five categories I spent far too much time naming:
- Novel and Novelty: `emma`, `persuasion`, `sense` 
- Bard's Tales: `caesar`, `macbeth`, `hamlet`
- Chestertomes: `ball`, `brown`, `thursday`
- BMW (Blake, Milton, Whitman): `poems`, `paradise`, `leaves`
- BBC (Bryant, Burgess, Carroll): `stories`, `buster`, `alice`

In other words, our modest library contains three Jane Austen's novels, three Shakespeare's plays, three novels by Gilbert K. Chesterton, three poem collections, and three children books (I'm sorry, Mr. Carroll). Let's find out if this classification is equally intuitive to a machine.

### Chapter Two: In which Books are Turned into Numbers and What Happens Then
There are different ways to represent a collection of documents as a set of numerical vectors. We're going to use term frequency-inverse document frequency (tf-idf). Technical details aside, the tf-idf score of a term in a document is largest when that term occurs frequently in that document, but is rare across all documents in the collection. 

```python
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(texts, max_df=.5, stop_words='english')
tfidf_matrix = vectorizer.fit_transform(texts)
```
```
```

The `TfidfVectorizer` does all the work for us – it filters stop words, normalizes every row of the tf-idf matrix, and lets us impose constraints on the value of document frequency (in our case, `max_df=.5` means thtat we ignore terms that occur in more than half of documents). The result is a matrix whose every column corresponds to a term and every row corresponds to a document. We can now find the term with the maximum tf-idf score for each document:

```python
terms = vectorizer.get_feature_names()
max_term = np.argmax(tfidf_matrix.toarray(), axis=1)
for index, document in enumerate(documents):
    print("{}: {}".format(document, terms[max_term[index]])
```

```
emma: emma
persuasion: anne
sense: elinor
poems: weep
stories: jackal
burgess-busterbrown: buster
alice: alice
ball: turnbull
brown: flambeau
thursday: syme
milton-paradise: hath
caesar: bru
hamlet: ham
macbeth: macb
leaves: states
```

For the most part, the results

[^1]: [Don't mention Macbeth](https://www.youtube.com/watch?v=h--HR7PWfp0) 
