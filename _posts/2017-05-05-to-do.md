---
layout:     post
title:      "Movable Type"
date:       2017-05-05
summary:    Playing around with books from project Gutenberg.
categories: nlp ml
published: true
---

## Prologue

Books are fun. <sup>[[citation needed](https://xkcd.com/285)]</sup> 

What's even more fun are vector space models, clustering algorithms, and dimensionality reduction techniques. In this post, we're going to combine it all by playing around with a small set of texts from project Gutenberg. With a bit of luck, Python, and lots of trial and error, we might just learn something interesting.

## Chapter One: In Which a Library is Created
First, we're going to need some books. Let's be lazy about it and just use what NLTK has to offer: 

```python
from nltk.corpus import gutenberg

fileids = gutenberg.fileids()
print(fileids)
```

```sh
austen-emma.txt
austen-persuasion.txt
austen-sense.txt
bible-kjv.txt
blake-poems.txt
bryant-stories.txt
burgess-busterbrown.txt
carroll-alice.txt
chesterton-ball.txt
chesterton-brown.txt
chesterton-thursday.txt
edgeworth-parents.txt
melville-moby_dick.txt
milton-paradise.txt
shakespeare-caesar.txt
shakespeare-hamlet.txt
shakespeare-macbeth.txt
whitman-leaves.txt
```

This rather eclectic collection will serve as our dataset. We can weed out some boring books and fetch the full text for the rest. Let's also be pedantic and use some regexp magic to format the titles:

```python
import re

boring = {'bible-kjv.txt',
          'edgeworth-parents.txt',
          'melville-moby_dick.txt'}

fileids = [f for f in fileids if f not in boring]
texts = [gutenberg.raw(f) for f in fileids]
titles = [re.search(r'-(.*?)\.', title).group(1) for title in titles]
```

Conveniently, and completely coincidentally, the remaining titles fall roughly into five categories I spent too much time naming:
 
- Novel and Novelty: *Emma, Persuasion, Sense and Sensibility*

- Bard's Tales: *Julius Caesar, Macbeth, Hamlet*

- Chestertomes: *The Ball and the Cross, The Innocence of Father Brown, The Man Who Was Thursday*

- BMW (Blake, Milton, Whitman): *Poems, Paradise Lost, Leaves of Grass*

- BBC (Bryant, Burgess, Carroll): *Stories, The Adventures of Buster Bear, Alice's Adventures in Wonderland*

In other words, our modest library contains three Jane Austen's novels, three Shakespeare's plays, three novels by Gilbert K. Chesterton, three poem collections, and three children books (I'm sorry, Mr. Carroll). Let's see if this classification is equally intuitive to a machine.

## Chapter Two: In which Books are Turned into Numbers
There are different ways to represent text documents as vectors. We're going to use term frequency-inverse document frequency (tf-idf). Technical details aside, the tf-idf score of a term in a document is largest when that term occurs frequently in that document, but is rare across all documents in the collection. 

```python
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(texts,
                             max_df=0.5,
                             stop_words='english')

tfidf_matrix = vectorizer.fit_transform(texts)
```

The TfidfVectorizer does all the work for us – it filters stop words, normalizes every row of the tf-idf matrix, and lets us impose constraints on document frequency. The result is a matrix with every column corresponding to a term and every row representing a document. We can now find the most important term in each book:

```python
terms = vectorizer.get_feature_names()
max_term = np.argmax(tfidf_matrix.toarray(), axis=1)

for index, document in enumerate(documents):
    print("{}: {}".format(document, terms[max_term[index]])
```

```sh
emma: emma
persuasion: anne
sense: elinor
poems: weep
stories: jackal
busterbrown: buster
alice: alice
ball: turnbull
brown: flambeau
thursday: syme
paradise: hath
caesar: bru
hamlet: ham
macbeth: macb
leaves: states
```
In case of novels and plays, main characters get highest tf-idf scores, although Brutus stole the show from the eponymous character of Shakespeare's Julius Caesar (*et tu, Brute?*). In case of lyric poetry, the results are slightly more revealing – it appears that Whitman really liked the States, while Blake's poems involve a fair amount of weeping. 

## Chapter Three: Wordclouds and How They Form 
For a more artistic take, we can use the tf-idf and the [wordcloud](https://github.com/amueller/word_cloud) library. Guessing the book behind every wordcloud is left as an exercise to the reader.

{% include image name="alice_wordcloud.png" width="500" caption="" %}

{% include image name="pride_wordcloud.png" width="500" caption="" %}

{% include image name="hamlet_wordcloud.png" width="500" caption="" %}

Poor Yorick tells us quite a lot about Shakespearean orthography. It seems that *i* and *j* were used interchangeably (*ioy*, *poyson*, *lye*), *u* and *v* could represent a vowel or a consonant, with *v* used at the beginning of a word (*vpon*, *haue*, *vs*), and silent final *e* was en vougue (*selfe*, *queene*, *meane*).

## Chapter Four: Let's Get Together!
Wordclouds look cool, but aren't terribly useful as a data visualization method.

[^1]: [Don't mention Macbeth](https://www.youtube.com/watch?v=h--HR7PWfp0) 
