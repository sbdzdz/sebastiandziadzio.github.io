---
layout:     post
title:      "Movable Type"
date:       2017-05-05
summary:    Playing around with books from project Gutenberg.
categories: nlp ml
custom_js:
    - katex
published: true
---

## Prologue

Books are fun. <sup>[[citation needed](https://xkcd.com/285)]</sup> 

What's even more fun are vector space models, clustering algorithms, and dimensionality reduction techniques. In this post, we're going to combine it all by playing around with a small set of texts from project Gutenberg. With a bit of luck, Python, and lots of trial and error, we might just learn something interesting.

## Chapter One: In Which a Library is Created
First, we're going to need some books. Let's be lazy about it and just use what NLTK has to offer: 

```python
from nltk.corpus import gutenberg

fileids = gutenberg.fileids()
print(', '.join(fileids))
```

```sh
austen-emma.txt, austen-persuasion.txt, austen-sense.txt, bible-kjv.txt, blake-poems.txt, bryant-stories.txt, burgess-busterbrown.txt, carroll-alice.txt, chesterton-ball.txt, chesterton-brown.txt, chesterton-thursday.txt, edgeworth-parents.txt, melville-moby_dick.txt, milton-paradise.txt, shakespeare-caesar.txt, shakespeare-hamlet.txt, shakespeare-macbeth.txt, whitman-leaves.txt
```

This rather eclectic collection will serve as our dataset. We can weed out the boring books and fetch the full text for the others. Let's also be pedantic and format the titles:

```python
import re

boring = {'bible-kjv.txt',
          'edgeworth-parents.txt',
          'melville-moby_dick.txt'}

fileids = [f for f in fileids if f not in boring]
texts = [gutenberg.raw(f) for f in fileids]
titles = [t.replace('.txt', '') for t in titles]
```

Conveniently, and completely coincidentally, the remaining titles fall roughly into five categories I spent too much time naming:
 
- Novel and Novelty: *Emma, Persuasion, Sense and Sensibility*

- Bard's Tales: *Julius Caesar, Hamlet, The Scottish Play*[^1]

- Chestertomes: *The Ball and the Cross, The Wisdom of Father Brown, The Man Who Was Thursday*

- BMW (Blake, Milton, Whitman): *Poems, Paradise Lost, Leaves of Grass*

- BBC (Bryant, Burgess, Carroll): *Stories, The Adventures of Buster Bear, Alice's Adventures in Wonderland*

In other words, our modest library contains three Jane Austen's novels, three Shakespeare's plays, three novels by Gilbert K. Chesterton, three poem collections, and three children books (I'm sorry, Mr Carroll). Let's see if this classification is equally intuitive to a machine.

## Chapter Two: In which Books are Turned into Numbers
There are different ways to represent text documents as vectors. We're going to use term frequency-inverse document frequency (tf-idf). Technical [details](https://en.wikipedia.org/wiki/Tf–idf) aside, the tf-idf score of a term in a document is largest when that term occurs frequently in that document, but is rare across all documents in the collection. 

```python
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(texts,
                             max_df=0.5,
                             stop_words='english')

tfidf_matrix = vectorizer.fit_transform(texts)
```

The `TfidfVectorizer` does all[^2] the work – it filters stop words, normalizes the term vectors and even lets us impose constraints on document frequency (`max_df=0.5` means we ignore terms that occur in more than half of the documents). The result is a matrix whose every column corresponds to a term and every row is a vector representing a book:

{% include image name="one_hot.png" width="600" caption=""%}

The `tfidf_matrix` is just a (sparse) numerical matrix, so we need to know which row/column corresponds to which book/term. The order of rows is the same as in `texts`, so that part is easy. Getting the list of terms is a bit more tricky:

```python
vocabulary = vectorizer.get_feature_names()
print ' '.join(vocabulary[:3])
```
```sh
00 000 0021053
```

The terms seem to be sorted alphabetically, so the beginning of the list is mostly numbers. We want to see some words!
```python
a = [t for t in terms if t.startswith('a')][:3]
z = [t for t in terms if t.startswith('z')][-3:]
print(' '.join(a))
print(' '.join(z))
```

```sh
aaron aback abandon abandoned abandoning
zoological zophiel zso zumpt zuyder
```

Nice! No *aardvarks* or *aardwolves*, but at least *Aaron* is there. We can now find top five terms for each book: 
```python
top_indices = (-tfidf_matrix).toarray().argsort()[:, :5]

for index, title in enumerate(titles):
    top_terms = [vocabulary[t] for t in top_indices[index]]
    top_terms = ', '.join(top_terms)
    print(f'{title}:\n{top_terms}\n')
```

```sh
austen-emma: 
emma harriet weston knightley elton

austen-persuasion:
anne elliot wentworth captain charles

austen-sense:
elinor marianne dashwood jennings willoughby

blake-poems:
weep thel infant lamb lyca

bryant-stories:
jackal margery nightingale big brahmin

burgess-busterbrown:
buster joe farmer blacky sammy

carroll-alice:
alice gryphon turtle hatter duchess

chesterton-ball:
turnbull macian evan police gutenberg

chesterton-brown:
flambeau muscari boulnois fanshaw duke

chesterton-thursday:
syme professor gregory marquis bull

milton-paradise:
hath adam eve satan spake

shakespeare-caesar:
brutus cassius caesar haue antony

shakespeare-hamlet:
hamlet haue horatio queene laertes

shakespeare-macbeth:
macbeth haue macduff rosse vpon

whitman-leaves:
states poems cities america chant
```

Our library is rather small, so dull words like *spoke* and *had* unfortunately made the cut, just because Milton and Shakespeare spelled them funny. This could be fixed by adding all the *haues*, *spakes*, *vpons*, and *haths* to the stop word list. Apart from that, the results are pretty informative, with character names generally scoring highest on tf-idf.

Curiously, it seems Brutus stole the show from the eponymous character of Shakespeare's Julius Caesar (*et tu, Brute?*) and Satan is more prominent than God in Paradise Lost. The latter is particularly interesting, as it sheds some light on how tf-idf works. A quick grep shows that 327 verses of Milton's masterpiece take the Lord's name in vain, while only 72 feature Satan, so good triumphs over evil as far as term frequency is concerned. However, Satan is mentioned less often in other books in our collection, so (s)he[^3] scores much higher on inverse document frequency. 

Lyric poetry features fewer characters, so only Blake's Lyca and Thel make it to the top five. Instead, we get an insight into poets' favourite topics – it appears Whitman was really into America, while Blake's poems involve lots of weeping lambs.


## Chapter Three: Wordclouds and How They Form 
For a more artistic take, let's visualize the tf-idf vectors as [wordclouds](https://github.com/amueller/word_cloud). I'll let you guess which is which.

{% include image name="pride_wordcloud.png" width="800" caption="" %}

{% include image name="alice_wordcloud.png" width="800" caption="" %}

{% include image name="paradise_wordcloud.png" width="800" caption="" %}

{% include image name="hamlet_wordcloud.png" width="800" caption="" %}

Poor Yorick tells us quite a lot about Shakespearean orthography. It seems *i*/*j* were used interchangeably (*ioy*, *poyson*, *lye*) and *u*/*v* could represent either a vowel or a consonant, with *v* used at the beginning of a word (*vpon*, *haue*, *vs*). Moreovere, it woulde seeme silente finale *ees* were quite populare.

## Chapter Four:

Wordclouds were a cool digression, but it's time to get back to the main question – what happens if we try to automatically classify our books into categories? We'll use the $$k$$-means clustering algorithm to find out. It starts with $$k$$ points (means), creates clusters by assigning each sample to the nearest mean (using squared Euclidean distance) and sets the means to be the centroids of the new clusters. After repeating this process a couple of times, the algorithm usually converges, although often to a local optimum. How do we choose $$k$$? In our case we just set it to five, since we want five clusters, but in general it's a problem deserving at least a [Wikipedia page](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set).

```python
from sklearn.cluster import KMeans
from collections import defaultdict

kmeans = KMeans(n_clusters=5)
labels = kmeans.fit_predict(tfidf_matrix)

clustering = defaultdict(list)
for title, label in zip(titles, labels):
    clustering[label].append(title)
    
for cluster, elements in sorted(clustering.items()):
    elements = ', '.join(elements)
    print(f'Cluster {label}: {elements}')
```
```sh
0: bryant-stories, burgess-busterbrown
1: austen-emma, austen-persuasion, austen-sense, chesterton-ball, chesterton-brown, chesterton-thursday
2: shakespeare-caesar, shakespeare-hamlet, shakespeare-macbeth
3: blake-poems, milton-paradise, whitman-leaves
4: carroll-alice
```
Not bad! First of all, I owe Mr [Dogdson](https://en.wikipedia.org/wiki/Lewis_Carroll) another apology. *Alice* is in a class of its own and doesn't belong with children stories. Instead, Austen and G.K. have to share a room. The rest is as expected – poets stick together[^4] and Shakespeare's tragedies are in one place. What happens if we use six clusters?

```sh
0: shakespeare-caesar, shakespeare-hamlet, shakespeare-macbeth
1: blake-poems, milton-paradise, whitman-leaves
2: austen-emma, austen-persuasion, austen-sense
3: bryant-stories, burgess-busterbrown
4: chesterton-ball, chesterton-brown, chesterton-thursday
5: carroll-alice
```
Chesterton and Austen now get a cluster of their own, but *Alice* is still lonely. Both with five and six clusters, something weird happens if we run the code a couple of times – we get different results! Remember the local optimum problem? We can try to avoid that by running the algorithm a couple of times and choosing the best result. Actually, this already happens behind the scenes: by default, the algorithm runs 10 times with different initial positions of the means and the solution with lowest inertia[^5] is returned. Unfortunately, we can still get unlucky and hit a sub-optimal solution every time. Let's run the algorithm 50 times and see if we find a better clustering.

```python
kmeans = KMeans(n_clusters=5, n_init=20)
```
```sh
0: blake-poems, milton-paradise, whitman-leaves
1: austen-emma, austen-persuasion, austen-sense
2: chesterton-ball, chesterton-brown, chesterton-thursday
3: shakespeare-caesar, shakespeare-hamlet, shakespeare-macbeth
4: bryant-stories, burgess-busterbrown, carroll-alice
```
Nice! Unless we got really unlucky, this is the best solution and these are exactly the categories we defined earlier. The final step would be to find a way to visualize the clusters. 

## Chapter Five: the Collapse of Space
There is an important problem with visualizing tf-idf vectors – they live in a high-dimensional space:

```python
print(len(terms))
```
```sh
28282
```
This is where dimensionality reduction comes in handy. People generally have trouble visualizing more than three dimensions [^6]. We'll use a clever technique called t-SNE to reduce our data into two dimensions, while hopefully retaining as much useful information as possible.

[^1]: One should never say the name of the [Scottish Play](https://www.youtube.com/watch?v=h--HR7PWfp0). It is unclear whether the curse extends to web publications, but I want to be on the safe side.
[^2]: Actually, that's not exactly true. I edited the texts a bit before feeding them into the vectorizer. For example, the names of characters in Shakespeare's plays are often abbreviated, so I replaced them with full versions to avoid duplicates. You'll run into problems like that on every text processing task and sometimes there's no way to fully automate it. It's usually a tradeoff between the desire for perfect results and the amount of fucks you're willing to give. 
[^3]: Ok I might [over-research](https://en.wikipedia.org/wiki/Sexuality_in_Christian_demonology) things.
[^4]: Those three have quite a lot in common. Blake was evidently a Milton's fanboy – he illustrated Milton's work more often than that of any other author and even wrote an epic poem called Milton, starring John Milton as a falling star entering Blake's foot ([I'm not making this up](https://en.wikipedia.org/wiki/Milton:_A_Poem_in_Two_Books)). Blake, in turn, was probably a major inspiration for Whitman, who even based the design of his own burial vault on Blake's engraving.
[^5]: Inertia, also known as within-cluster sum of squares, is the sum of squared Euclidean distances of samples to their closest cluster center. Note that distance is more of an intuition here, what $$k$$-means really minimizes is within-cluster variance. They're the same in this case, but we don't want anyone from the Internet to "*well, actually*" us, do we?
[^6]: Mathematicians like to claim it is in fact trivial. To imagine a four-dimensional space, you first imagine an $$n$$-dimensional space and then simply set $$n=4$$.
