---
layout:     post
title:      "No Title"
date:       2017-05-05
summary:    Playing around with books from project Gutenberg.
categories: nlp ml
custom_js:
    - katex
---

### Prologue: In Which the Author Explains Why He's Doing What He's Doing

Books are fun. <sup>[[citation needed](https://xkcd.com/285)]</sup> 

What's even more fun are vector space models, clustering algorithms, and dimensionality reduction techniques. In this blog post, we're going to combine it all by playing around with a small set of texts from project Gutenberg. With a bit of luck, Python, and lots of trial and error, we might just learn something interesting.

### Chapter One: In Which Books are Fetched and Puns are Made
We should start by fetching some books. There are many ways to do it, but for starters let's just use what NLTK has to offer: 

```python
>>> from nltk.corpus import gutenberg
>>> titles = gutenberg.fileids()
>>> titles
['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt',
 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt',
 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt',
 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt',
 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt',
 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']
```

This rather eclectic collection will serve as our dataset. How about we weed out some boring books and get the full text for the rest (your definition of boring may vary):

```python
>>> boring = {'bible-kjv.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt'} 
>>> titles = [t for t in titles if t not in boring] 
>>> texts = [gutenberg.raw(t) for t in titles] 
```

Conveniently (and completely coincidentally) the remaining titles fall into five categories I spent far too much time naming:
- Novel and Novelty: `emma`, `persuasion`, `sense` 
- Bard's Tales: `caesar`, `macbeth`, `hamlet`
- Chestertomes: `ball`, `brown`, `thursday`
- BMW (Blake, Milton, Whitman): `poems`, `paradise`, `leaves`
- BBC (Bryant, Burgess, Carroll): `stories`, `buster`, `alice`

In other words, our modest library contains three Jane Austen's novels, three Shakespeare's plays, three novels by Gilbert K. Chesterton, three poem collections, and three children books (I'm sorry, Mr. Carroll). Let's find out if this classification is equally intuitive to a machine.

### Chapter Two: In which Books are Magically Turned into Numbers and What Happens Then
Before we can run any machine learning algorithms on text documents, we need to find a way to represent them as vectors. An ideal representation would distribute documents in a multi-dimensional vector space so that similar documents are close to each other (for now let's not worry about what "similar" or "close" means). One idea could be to construct a following term-document matrix:

{% include image name="one_hot.png" width="500" caption=""%}

Each row of the matrix represents a document and each column corresponds to a term. The values are just raw counts of terms in respective documents. In the example above, "Emma" contains one occurence of *aardvark* and two occurences of *aardwolf*, but no occurences of *zulu*, while "Alice in Wonderland" features surprisingly many Zulus and no aard-creatures. 

Now this is a pretty awful idea. First of all, we are using raw counts, so longer documents will result in much larger values. Even if adjust for the length of document, the most frequent terms will likely be *the*, *be*, *to*, *of*, and the like, so all the vectors will end up pretty similar. What we need is a cunning way to discount unimportant terms and bump up relevant ones. Intuitively, a term is relevant to a document if it occurs frequently in this document, but is rare across all documents. Slightly more formally, given a collection of documents $$D$$, we would like to assign to each term $$t$$ in document $$d \in D$$ a weight $$w_{t, d}$$ that is:

* proportional to the frequency of $$t$$ in $$d$$,
* inversely proportional to the percentage of documents in D containing $$t$$.

Just by formulating the problem, we accidentaly discovered a weighting scheme aptly named *term frequency-inverse document frequency*: 

$$ \text{tf-idf}(t, d, D) = \text{tf}(t, d) \cdot \text{idf}(t, D) $$

In the most popular variant, the first factor is just the count of term $$t$$ in document $$d$$ divided by the length of the document:

$$ \text{tf}(t, d)=\frac{n_{t, d}}{\sum_{t' \in d}n_{t', d}} $$

The second factor is usually taken to be the logarithm of the total number of documents divided by the number of documents containing $$t$$:

$$ \text{idf}(t, D)=\frac{|D|}{|{d \in D: t \in d}|} $$

If you find the above notation to be unnecessary cryptic, here's a quick example:

```python
>>> d1 = 'egg bacon sausage and spam'
>>> d2 = 'spam bacon sausage and spam'
>>> d3 = 'spam egg spam spam bacon and spam'
>>> D = [d1, d2, d3]
```
Say we want to find the tf-idf weight for all the terms in the second document. Let's start with term frequencies:
```python
>>> from collections import Counter 
>>> c = Counter(d2.split())
>>> total = sum(c.values())
>>> term_frequencies = {t: counter[t]/total for t in counter}
>>> print(term_frequencies)

{'sausage': 0.2, 'bacon': 0.2, 'spam': 0.4, 'and': 0.2}
```

[^1]: [Don't mention Macbeth](https://www.youtube.com/watch?v=h--HR7PWfp0) 
