---
layout:     post
title:      "Movable Type"
date:       2017-05-05
summary:    Playing around with books from project Gutenberg.
categories: nlp ml
custom_js:
    - katex
published: true
---

### Prologue

Books are fun. <sup>[[citation needed](https://xkcd.com/285)]</sup> 

What's even more fun are vector space models, clustering algorithms, and dimensionality reduction techniques. In this blog post, we're going to combine it all by playing around with a small set of texts from project Gutenberg. With a bit of luck, Python, and lots of trial and error, we might just learn something interesting.

### Chapter One: In Which a Library is Created 
We should start by fetching some books. There are many ways to do it, but for starters let's just use what NLTK has to offer: 

```python
from nltk.corpus import gutenberg

fileids = gutenberg.fileids()
print(fileids)
```
```
['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt',
 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt',
 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt',
 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt',
 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt',
 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']
```

This rather eclectic collection will serve as our dataset. We can weed out some boring books and get the full text for the rest. Let's also be pedantic and use some regexp magic to format the titles:

```python
import re

boring = {'bible-kjv.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt'}
fileids = [f for f in fileids if f not in boring]
texts = [gutenberg.raw(f) for f in fileids]
titles = [re.search(r'-(.*?)\.', title).group(1) for title in titles]
print(titles)
```
```
['emma', 'persuasion', 'sense', 'poems', 'stories', 'busterbrown', 'alice', 'ball', 'brown', 'thursday', 'paradise', 'caesar', 'hamlet', 'macbeth', 'leaves']
```

Conveniently, and completely coincidentally, the remaining titles fall into five categories I spent far too much time naming:
- Novel and Novelty: `emma`, `persuasion`, `sense` 
- Bard's Tales: `caesar`, `macbeth`, `hamlet`
- Chestertomes: `ball`, `brown`, `thursday`
- BMW (Blake, Milton, Whitman): `poems`, `paradise`, `leaves`
- BBC (Bryant, Burgess, Carroll): `stories`, `buster`, `alice`

In other words, our modest library contains three Jane Austen's novels, three Shakespeare's plays, three novels by Gilbert K. Chesterton, three poem collections, and three children books (I'm sorry, Mr. Carroll). Let's find out if this classification is equally intuitive to a machine.

### Chapter Two: In which Books are Turned into Numbers
There are different ways to represent a collection of documents as a set of numerical vectors. We're going to use term frequency-inverse document frequency (tf-idf). Technical details aside, the tf-idf score of a term in a document is largest when that term occurs frequently in that document, but is rare across all documents in the collection. 

```python
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(texts, max_df=.5, stop_words='english')
tfidf_matrix = vectorizer.fit_transform(texts)
```
```
```

The TfidfVectorizer does all the work for us – it filters stop words, normalizes every row of the tf-idf matrix, and lets us impose constraints on document frequency. The result is a matrix with every column corresponding to a term and every row representing a document. We can now find the most important term in each book:

```python
terms = vectorizer.get_feature_names()
max_term = np.argmax(tfidf_matrix.toarray(), axis=1)
for index, document in enumerate(documents):
    print("{}: {}".format(document, terms[max_term[index]])
```

```
emma: emma
persuasion: anne
sense: elinor
poems: weep
stories: jackal
busterbrown: buster
alice: alice
ball: turnbull
brown: flambeau
thursday: syme
paradise: hath
caesar: bru
hamlet: ham
macbeth: macb
leaves: states
```

### Chapter Three: In Which Wordclouds Form

Unsurprisingly, in case of novels and drama, the highest tf-idf score is assigned to the name of one of the characters (abbreviated in case of Shakespeare's plays). One interesting observation is that Brutus stole the show from the eponymous character of Shakespeare's Julius Caesar (*et tu, Brute?*). In case of lyric poetry, the results are slightly more informative – it appears that Whitman really liked the States, while Blake's poems involve a fair amount of weeping. For a more detailed look, let's use the tf-idf scores to build a wordcloud for selected texts.

{% include image name="alice_wordcloud.png" height="300" caption=""%}

[^1]: [Don't mention Macbeth](https://www.youtube.com/watch?v=h--HR7PWfp0) 
