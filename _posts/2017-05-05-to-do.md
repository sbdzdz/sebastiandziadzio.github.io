---
layout:     post
title:      "Movable Type"
date:       2017-05-05
summary:    Playing around with books from project Gutenberg.
categories: nlp ml
published: true
---

## Prologue

Books are fun. <sup>[[citation needed](https://xkcd.com/285)]</sup> 

What's even more fun are vector space models, clustering algorithms, and dimensionality reduction techniques. In this post, we're going to combine it all by playing around with a small set of texts from project Gutenberg. With a bit of luck, Python, and lots of trial and error, we might just learn something interesting.

## Chapter One: In Which a Library is Created
First, we're going to need some books. Let's be lazy about it and just use what NLTK has to offer: 

```python
from nltk.corpus import gutenberg

fileids = gutenberg.fileids()
print(', '.join(fileids))
```

```sh
austen-emma.txt, austen-persuasion.txt, austen-sense.txt, bible-kjv.txt, blake-poems.txt, bryant-stories.txt, burgess-busterbrown.txt, carroll-alice.txt, chesterton-ball.txt, chesterton-brown.txt, chesterton-thursday.txt, edgeworth-parents.txt, melville-moby_dick.txt, milton-paradise.txt, shakespeare-caesar.txt, shakespeare-hamlet.txt, shakespeare-macbeth.txt, whitman-leaves.txt
```

This rather eclectic collection will serve as our dataset. We can weed out the boring books and fetch the full text for the others. Let's also be pedantic and format the titles:

```python
import re

boring = {'bible-kjv.txt',
          'edgeworth-parents.txt',
          'melville-moby_dick.txt'}

fileids = [f for f in fileids if f not in boring]
texts = [gutenberg.raw(f) for f in fileids]
titles = [t.replace('.txt', '') for t in titles]
```

Conveniently, and completely coincidentally, the remaining titles fall roughly into five categories I spent too much time naming:
 
- Novel and Novelty: *Emma, Persuasion, Sense and Sensibility*

- Bard's Tales: *Julius Caesar, Hamlet, The Scottish Play[^1]*

- Chestertomes: *The Ball and the Cross, The Wisdom of Father Brown, The Man Who Was Thursday*

- BMW (Blake, Milton, Whitman): *Poems, Paradise Lost, Leaves of Grass*

- BBC (Bryant, Burgess, Carroll): *Stories, The Adventures of Buster Bear, Alice's Adventures in Wonderland*

In other words, our modest library contains three Jane Austen's novels, three Shakespeare's plays, three novels by Gilbert K. Chesterton, three poem collections, and three children books (I'm sorry, Mr. Carroll). Let's see if this classification is equally intuitive to a machine.

## Chapter Two: In which Books are Turned into Numbers
There are different ways to represent text documents as vectors. We're going to use term frequency-inverse document frequency (tf-idf). Technical [details](https://en.wikipedia.org/wiki/Tf–idf) aside, the tf-idf score of a term in a document is largest when that term occurs frequently in that document, but is rare across all documents in the collection. 

```python
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(texts,
                             max_df=0.5,
                             stop_words='english')

tfidf_matrix = vectorizer.fit_transform(texts)
```

The `TfidfVectorizer` does all[^2] the work – it filters stop words, normalizes the term vectors and even lets us impose constraints on document frequency (`max_df=0.5` means we ignore terms that occur in more than half of the documents). The result is a matrix with every column corresponding to a term and every row representing a book:

{% include image name="one_hot.png" width="600" caption=""%}

Let's have a peek on the terms in our vocabulary.
```python
vocabulary = vectorizer.get_feature_names()
print(' '.join(vocabulary[-5:]))
```
```sh
zoological zophiel zso zumpt zuyder
```

We can now find top five terms for each book. 
```python
top_indices = (-tfidf_matrix).toarray().argsort()[:, :5]

for index, title in enumerate(titles):
    top_terms = [vocabulary[t] for t in top_indices[index]]
    print("{}:\n{}\n".format(title, ' '.join(top_terms)))
```

```sh
austen-emma: 
emma harriet weston knightley elton

austen-persuasion:
anne elliot wentworth captain charles

austen-sense:
elinor marianne dashwood jennings willoughby

blake-poems:
weep thel infant lamb lyca

bryant-stories:
jackal margery nightingale big brahmin

burgess-busterbrown:
buster joe farmer blacky sammy

carroll-alice:
alice gryphon turtle hatter duchess

chesterton-ball:
turnbull macian evan police gutenberg

chesterton-brown:
flambeau muscari boulnois fanshaw duke

chesterton-thursday:
syme professor gregory marquis bull

milton-paradise:
hath adam eve satan spake

shakespeare-caesar:
brutus cassius caesar haue antony

shakespeare-hamlet:
hamlet haue horatio queene laertes

shakespeare-macbeth:
macbeth haue macduff rosse vpon

whitman-leaves:
states poems cities america chant
```

Our library is rather small, so dull words like *spoke* and *had* unfortunately made the cut, just because Milton and Shakespeare spelled them funny. This could be fixed by adding all the *haues*, *spakes*, *vpons*, and *haths* to the stop word list. Apart from that, the results are pretty informative, with character names generally scoring highest on tf-idf.

Curiously, it seems Brutus stole the show from the eponymous character of Shakespeare's Julius Caesar (*et tu, Brute?*) and Satan is more prominent than God in Paradise Lost. The latter is particularly interesting, as it sheds some light on how tf-idf works. A quick grep shows that 327 verses of Milton's masterpiece take the Lord's name in vain, while only 72 feature Satan, so good triumphs over evil as far as term frequency is concerned. However, Satan is mentioned less often in other books in our collection, so (s)he[^3] scores much higher on inverse document frequency. 

Lyric poetry features fewer characters, so only Blake's Lyca and Thel make it to the top five. Instead, we get an insight into poets' favourite topics – it appears Whitman was really into America, while Blake's poems involve lots of weeping lambs.


## Chapter Three: Wordclouds and How They Form 
For a more artistic take, let's visualize the tf-idf vectors as [wordclouds](https://github.com/amueller/word_cloud). I'll let you guess which is which.

{% include image name="pride_wordcloud.png" width="800" caption="" %}

{% include image name="alice_wordcloud.png" width="800" caption="" %}

{% include image name="paradise_wordcloud.png" width="800" caption="" %}

{% include image name="hamlet_wordcloud.png" width="800" caption="" %}

Poor Yorick tells us quite a lot about Shakespearean orthography. It seems *i*/*j* were used interchangeably (*ioy*, *poyson*, *lye*) and *u*/*v* could represent either a vowel or a consonant, with *v* used at the beginning of a word (*vpon*, *haue*, *vs*). Moreovere, it woulde seeme silente finale *ees* were quite populare.

## Chapter Four: Let's Get Together!
Wordclouds look cool, but aren't terribly useful as a data visualization method. However, there is an important problem with visualizing tf-idf vectors – they live in a high-dimensional space:

```python
print(len(terms))
```
```sh
28282
```

[^1]: [Don't mention Macbeth](https://www.youtube.com/watch?v=h--HR7PWfp0) 
[^2]: Actually, that's not exactly true. I edited the texts a bit before feeding them into the vectorizer. For example, the names of characters in Shakespeare's plays are often abbreviated, so I replaced them with full versions to avoid duplicates. You'll run into problems like that on every text processing task and sometimes there's no way to fully automate it. It's usually a tradeoff between the desire for perfect results and the amount of fucks you're willing to give. 
[^3]: [Ok I might over-research things](https://en.wikipedia.org/wiki/Sexuality_in_Christian_demonology)
